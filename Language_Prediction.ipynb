{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import Levenshtein\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_array = []\n",
    "english_array = []\n",
    "with open(\"hindi_english_transliteration.txt\") as f:\n",
    "    for line in f:\n",
    "        key, val = line.split()\n",
    "        hindi_array.append(key)\n",
    "\n",
    "with open(\"20k.txt\") as f:\n",
    "    for line in f:\n",
    "        key = line.split()\n",
    "        english_array.append(key[0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(hindi_array)\n",
    "random.shuffle(english_array)\n",
    "Train_Data_Hindi = hindi_array[:18000]\n",
    "Train_Data_English = english_array[:18000]\n",
    "Test_Data_Hindi = hindi_array[28815:]\n",
    "Test_Data_English = english_array[18000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hindi = {}\n",
    "count_english = {}\n",
    "for data in Train_Data_Hindi:\n",
    "    if data not in count_hindi.keys():\n",
    "        count_hindi[data] = 0\n",
    "    count_hindi[data]+=1\n",
    "for data in Train_Data_English:\n",
    "    if data not in count_english.keys():\n",
    "        count_english[data] = 0\n",
    "    count_english[data]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del count_english['hai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Training = dict()\n",
    "for data in Train_Data_English:\n",
    "    if data not in Data_Training.keys():\n",
    "        Data_Training[data] = 1\n",
    "for data in Train_Data_Hindi:\n",
    "    if data not in Data_Training.keys():\n",
    "        Data_Training[data] = 0\n",
    "Data_Testing = Test_Data_English +  Test_Data_Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(Data_Training , Data_Testing):\n",
    "    y_pred = []\n",
    "    for data in Data_Testing:\n",
    "        data_train = Data_Training.keys()\n",
    "        data_train = list(data_train)\n",
    "        random.shuffle(data_train)\n",
    "        l = len(data)\n",
    "        min_edit_dict = dict()\n",
    "        if data in count_english.keys():\n",
    "            y_pred.append(1)\n",
    "        elif data in count_hindi.keys():\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            for k in data_train:\n",
    "                set_result = set.intersection(set(data), set(k))\n",
    "                if len(set_result) > l/2 and abs(len(k)-len(data))/max(len(k),len(data)) < 0.6:\n",
    "                    if k not in min_edit_dict.keys():\n",
    "                        min_edit_dict[Levenshtein.distance(str(data),str(k))] = Data_Training[k]\n",
    "            od = collections.OrderedDict(sorted(min_edit_dict.items()))\n",
    "            od = list(od.values())\n",
    "            y_pred.append(1) if od[:5].count(1)>od[:5].count(0) else y_pred.append(0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86075"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = KNN(Data_Training , Data_Testing)\n",
    "(y_pred[:len(Test_Data_English)].count(1) + y_pred[len(Test_Data_English):].count(0))/len(Data_Testing) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN(Data_Training , ['ok' , 'then' ,  'talk' , 'to' , 'you' , 'later' , 'abhi' , 'thoda' , 'kam' , 'hai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Train_Array = []\n",
    "SVM_label = []\n",
    "SVM_Train_List = []\n",
    "with open(\"Hindi_English_FB.txt\") as f:\n",
    "    for line in f:\n",
    "        if line == '\\n':\n",
    "            SVM_Train_List.append(SVM_Train_Array)\n",
    "            SVM_Train_Array = []\n",
    "        else:\n",
    "            key , label , encod = line.split()\n",
    "            if label == 'en' or label == 'hi':\n",
    "                SVM_Train_Array.append(key.lower())\n",
    "                if label == 'en':\n",
    "                    SVM_label.append(1)\n",
    "                else:\n",
    "                    SVM_label.append(0)\n",
    "SVM_Train_List.append(SVM_Train_Array)\n",
    "                    \n",
    "SVM_Train_Array = []\n",
    "with open(\"Hindi_english.txt\") as f:\n",
    "    for line in f:\n",
    "        if line == '\\n':\n",
    "            SVM_Train_List.append(SVM_Train_Array)\n",
    "            SVM_Train_Array = []\n",
    "        else:\n",
    "            key , label , encod = line.split()\n",
    "            if label == 'en' or label == 'hi':\n",
    "                SVM_Train_Array.append(key.lower())\n",
    "                if label == 'en':\n",
    "                    SVM_label.append(1)\n",
    "                else:\n",
    "                    SVM_label.append(0)\n",
    "\n",
    "SVM_Train_Array = []\n",
    "with open(\"Hindi_English_Twitter.txt\") as f:\n",
    "    for line in f:\n",
    "        if line == '\\n':\n",
    "            SVM_Train_List.append(SVM_Train_Array)\n",
    "            SVM_Train_Array = []\n",
    "        else:\n",
    "            key , label , encod = line.split()\n",
    "            if label == 'en' or label == 'hi':\n",
    "                SVM_Train_Array.append(key.lower())\n",
    "                if label == 'en':\n",
    "                    SVM_label.append(1)\n",
    "                else:\n",
    "                    SVM_label.append(0)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Training_model = Word2Vec(SVM_Train_List , window = 5,size = 100,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovish/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "/home/lovish/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "SVM_Training = []\n",
    "SVM_Testing = []\n",
    "for i in SVM_Train_List[:2500]:\n",
    "    for j in i:\n",
    "        SVM_Training.append(SVM_Training_model[j])\n",
    "for i in SVM_Train_List[2500:]:\n",
    "    for j in i:\n",
    "        SVM_Testing.append(SVM_Training_model[j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(SVM_Training , SVM_label,SVM_Testing):\n",
    "    clf = LinearSVC(C = 5)\n",
    "    clf.fit(SVM_Training,SVM_label[:30532])\n",
    "    y_pred = clf.predict(SVM_Testing)\n",
    "    check = SVM_label[30532:]\n",
    "    count = 0\n",
    "    for j in range(len(y_pred)):\n",
    "        if y_pred[j] == check[j]:\n",
    "            count+=1\n",
    "    return count/len(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6811819595645412"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM(SVM_Training,SVM_label,SVM_Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest(SVM_Training , SVM_label,SVM_Testing):\n",
    "    clf1  = RandomForestClassifier(n_estimators=206, random_state=16)\n",
    "    clf1.fit(SVM_Training,SVM_label[:30532])\n",
    "    y_pred1 = clf1.predict(SVM_Testing)\n",
    "    count = 0\n",
    "    check = SVM_label[30532:]\n",
    "    for j in range(len(y_pred1)):\n",
    "        if y_pred1[j] == check[j]:\n",
    "            count+=1\n",
    "    return count/len(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8491446345256609"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random_Forest(SVM_Training , SVM_label,SVM_Testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
